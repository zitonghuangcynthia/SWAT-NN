# SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space

This is the official repository for our paper:
**"SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space"**  
[View on arXiv](https://arxiv.org/abs/2506.08270)

## Overview

This repository contains all code and scripts used in our experiments on simultaneous weight and architecture optimization of neural networks using a GPT2-based autoencoder framework. The method encodes MLPs into a latent space and jointly optimizes neuron activations, layer connectivity, and weight sparsity.

The workflow is divided into several stages:

1. **Train the AutoEncoder** on synthetic datasets of MLPs.
2. **Train sparse MLPs** on real-world or benchmark datasets (e.g., CORNN), starting from embeddings generated by the AutoEncoder.
3. (Optional) **Compress large pre-trained networks** using the same latent-space embedding + optimization framework.


## Quick Start

Install dependencies:

```bash
pip install -r requirements.txt
```


## Train autoencoder

Navigate to the autoencoder directory and train the model:
```
cd train_autoencoder
python main.py --cuda 0
```


## Train MLPs on Benchmark Dataset

Navigate to the autoencoder directory and train the model:
```
cd train_mlp
python train_mlp.py --cuda 0
```

### Dataset

This training uses the [CORNN dataset](https://github.com/CWCleghornAI/CORNN.git) as the benchmark for functional approximation tasks.

We **do not host the dataset in this repository**. Please follow the instructions in the official [CORNN GitHub repository](https://github.com/CWCleghornAI/CORNN.git) to download and prepare the dataset.

In our code, we assume that the CORNN dataset is available as a Python module (e.g., `lib/CORNN.py`) in the codebase, or properly installed and imported.

